# Lecture 17.02.2020

- geometry
- we used 2D shapes in three dimensions
- in OpenGL we have 3D space -- how can we define 3D shapes?
- we can define these things by defining them the same way as 2D shapes
- we specify the triangles that make up the faces of the body
- right now we would need to specify shared vertices each and every time
- this is not efficient -- we waste a lot of space and if we have more complex
shapes it becomes super messy
- we can specify shapes by two arrays in OpenGL -- one that just contains all
the points and a different one that just defines vertices 
- right now we use 7 floats for each vertex of our triangles -- if we make
a cube out of triangles there will be a lot of wasted space if we do not share
the data
- we save all the indices with their corresponding coordinates
- then, we make lists of indices that correspond to faces of the bodies
- thus we reuse data and save space, especially if we use many vertices or if
we store more than 7 values per vertex -- 100 can be stored in certain engines
- `reinperpret_cast` is unsafe because we assume that vertex data is stored the
same way as in an array -- we can't necessarily know if that is the case
- `vertex_array_object` contains a description of what our data looks like
because we could in theory be sending all kinds of stuff to the GPU
- in `es2_geometry.hpp` we send stuff to the GPU and tell it how the data is
structured
- we now have a `vertex.hpp` class and that contains `glm::vec3 position`,
`glm::vec4 color`, `glm::vec3 normal`, and `glm::vec4 texture_coordinates`
- normals are vectors that are perpendicular to the surface of our geometry
- normals are generally used to calculate light, to figure out which surfaces
should be lit up or not
- in `geometry.hpp` we now have an unsigned int vector that stores the indices
of the vertices
- we will just store all the indices one after another and then let the GPU
figure out that three in a row make a point or whatever else
- `es2_geometry.hpp` now has more than just the buffer, vertices and stuff are
stored by the CPU while `GLuint index_buffer_object` are stored by the GPU
- we need to write some code in that class that only uses the indices and
hopefully saves some memory -- we need to update the `stride` and the other
values -- how many elements we need to skip to get to the next element, how to
navigate the vertices
- most important call is `glDrawArrays` which actually draws all that stuff --
now we use `glDrawElements` which uses the index approach instead of the vertex
approach of `glDrawArrays`
- we will need small particles and circles in the future and thus
`geometry_generators` was moved to a different namespace so that we can reuse
it when we need to 
- we have `GL_POINTS`, `GL_LINES`, `GL_LINE_STRIP`, `GL_LINE_LOOP`,
`GL_TRIANGLES`, `GL_TRIANGLE_STRIP`, `GL_TRIANGLE_FAN`
- when we generate spheres for example we can string together triangles in
rings to make a sphere __see recording of the lecture__
- desktop GPUs support `GL_QUAD_STRIP` which strips together polygons of
4 vertices -- it is supported by desktop GPUs but not by mobile GPUs -- it is
not often used because of that and it also comes with performance penaties
